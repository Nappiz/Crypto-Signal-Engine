{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import & Configuration"
      ],
      "metadata": {
        "id": "VJpxqPO4jfxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mIKxlr_jd8h"
      },
      "outputs": [],
      "source": [
        "!pip install ccxt -q\n",
        "\n",
        "import ccxt\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "EXCHANGE_ID = 'bitstamp'\n",
        "SYMBOL = 'XRP/USD'\n",
        "TIMEFRAME = '1h'\n",
        "YEARS_BACK  = 6\n",
        "RETRY_LIMIT = 3\n",
        "SLEEP_TIME  = 0.5\n",
        "\n",
        "print(\"Import & Configuration Ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DATA ENGINEERING\n",
        "---"
      ],
      "metadata": {
        "id": "q8h-uppAq5CK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miner"
      ],
      "metadata": {
        "id": "_b-COTstjjUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_exchange(exchange_id):\n",
        "    try:\n",
        "        exchange_class = getattr(ccxt, exchange_id)\n",
        "        exchange = exchange_class({\n",
        "            'enableRateLimit': True,\n",
        "        })\n",
        "        return exchange\n",
        "    except AttributeError:\n",
        "        print(f\"‚ùå Exchange '{exchange_id}' not found.\")\n",
        "        return None\n",
        "\n",
        "def fetch_historical_data_robust(symbol, timeframe, years, exchange):\n",
        "    now = exchange.milliseconds()\n",
        "    start_time = now - (years * 365 * 24 * 60 * 60 * 1000)\n",
        "\n",
        "    print(f\"üîÑ Target Mining: {pd.to_datetime(start_time, unit='ms')} until Now\")\n",
        "    print(f\"   Exchange: {exchange.id.upper()}\")\n",
        "\n",
        "    all_ohlcv = []\n",
        "    current_since = start_time\n",
        "\n",
        "    while current_since < now:\n",
        "        # --- LOGIC RETRY ---\n",
        "        success = False\n",
        "        ohlcv = []\n",
        "\n",
        "        for attempt in range(RETRY_LIMIT):\n",
        "            try:\n",
        "                ohlcv = exchange.fetch_ohlcv(symbol, timeframe, since=current_since, limit=1000)\n",
        "                success = True\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error (Attempt {attempt+1}/{RETRY_LIMIT}): {e}\")\n",
        "                time.sleep(2 * (attempt + 1))\n",
        "\n",
        "        if not success:\n",
        "            print(\"‚ùå Complete failure after several attempts. Stopping.\")\n",
        "            break\n",
        "\n",
        "        if not ohlcv:\n",
        "            print(\"‚ö†Ô∏è Warning: Exchange stopped giving data (EOF).\")\n",
        "            break\n",
        "\n",
        "        # --- SANITY CHECK ---\n",
        "        first_candle_time = ohlcv[0][0]\n",
        "        time_diff_days = (first_candle_time - current_since) / (1000 * 60 * 60 * 24)\n",
        "\n",
        "        if time_diff_days > 30 and len(all_ohlcv) == 0:\n",
        "            print(f\"‚ùå CRITICAL: Exchange ignores history parameters.\")\n",
        "            print(f\"   Diminta: {pd.to_datetime(current_since, unit='ms')}\")\n",
        "            print(f\"   Dikasih: {pd.to_datetime(first_candle_time, unit='ms')}\")\n",
        "            return []\n",
        "\n",
        "        all_ohlcv.extend(ohlcv)\n",
        "\n",
        "        # Update pointer\n",
        "        last_timestamp = ohlcv[-1][0]\n",
        "        last_date_human = pd.to_datetime(last_timestamp, unit='ms')\n",
        "        print(f\"   -> Progress: {last_date_human} | Total: {len(all_ohlcv)} rows\")\n",
        "\n",
        "        if last_timestamp >= current_since:\n",
        "             current_since = last_timestamp + 1\n",
        "        else:\n",
        "             current_since += (1000 * 60 * 60)\n",
        "\n",
        "        time.sleep(SLEEP_TIME)\n",
        "\n",
        "    return all_ohlcv\n",
        "\n",
        "print(\"Starting Data Mining...\")\n",
        "exchange = init_exchange(EXCHANGE_ID)\n",
        "\n",
        "if exchange:\n",
        "    raw_data = fetch_historical_data_robust(SYMBOL, TIMEFRAME, YEARS_BACK, exchange)\n",
        "    print(f\"Done! Collected {len(raw_data)} raw candles.\")\n",
        "else:\n",
        "    print(\"Failed to initialize exchange.\")\n",
        "    raw_data = []"
      ],
      "metadata": {
        "id": "MQJvgLMgjwTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation & Storage"
      ],
      "metadata": {
        "id": "GIQQIjaTrYcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_validate(raw_data_list):\n",
        "    if not raw_data_list:\n",
        "        print(\"Empty Data/Empty List. Cek the output of 2nd Cell.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nCleaning Started...\")\n",
        "\n",
        "    # Convert ke DataFrame\n",
        "    df = pd.DataFrame(raw_data_list, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
        "\n",
        "    # Convert Timestamp & Set Index\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    # Deduplicate\n",
        "    initial_len = len(df)\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "    dedup_len = len(df)\n",
        "\n",
        "    if initial_len != dedup_len:\n",
        "        print(f\"   -> Founded & Deleted {initial_len - dedup_len} Duplicated Row.\")\n",
        "    else:\n",
        "        print(\"   -> No Duplicate. Clean data.\")\n",
        "\n",
        "    df.sort_index(inplace=True)\n",
        "\n",
        "    print(\"\\n--- DATA QUALITY REPORT ---\")\n",
        "    print(f\"Range Data  : {df.index.min()} until {df.index.max()}\")\n",
        "    print(f\"Total Duration: {(df.index.max() - df.index.min()).days} Days\")\n",
        "    print(f\"Total Row : {len(df)}\")\n",
        "\n",
        "    missing = df.isnull().sum().sum()\n",
        "    if missing > 0:\n",
        "        print(f\"‚ö†Ô∏è WARNING: There's {missing} NaN!\")\n",
        "    else:\n",
        "        print(\"Complete Data (No Missing Values).\")\n",
        "\n",
        "    return df\n",
        "\n",
        "if 'raw_data' in locals() and raw_data:\n",
        "    df_engine = process_and_validate(raw_data)\n",
        "\n",
        "    if df_engine is not None:\n",
        "        filename = f\"raw_{SYMBOL.replace('/', '')}_{TIMEFRAME}.csv\"\n",
        "        df_engine.to_csv(filename)\n",
        "        print(f\"\\nSUCCESS. Dataset saved: {filename}\")\n",
        "else:\n",
        "    print(\"'raw_data' Variable not found or null. Run the 2nd Cell first!\")"
      ],
      "metadata": {
        "id": "X9noz35dracL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DATA ANALYSIS\n",
        "---"
      ],
      "metadata": {
        "id": "gCCXVKJ6rFEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load"
      ],
      "metadata": {
        "id": "P8uG-k_-jnPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = f\"raw_{SYMBOL.replace('/', '')}_{TIMEFRAME}.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(filename)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    print(\"Data Loaded Successfully!\")\n",
        "    print(f\"Periode: {df.index.min()} s/d {df.index.max()}\")\n",
        "    print(f\"Total Duration: {(df.index.max() - df.index.min()).days} Days\")\n",
        "    print(f\"Total Row: {len(df)}\")\n",
        "\n",
        "    print(df.info())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"There is no file named {filename}. Run the Data Engineering first.\")"
      ],
      "metadata": {
        "id": "mCs8-0BRjw0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "kKHyjRkgjpFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (20, 30)\n",
        "plt.rcParams['font.family'] = 'sans-serif'\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "\n",
        "# --- A. DATA PREPARATION ---\n",
        "eda_df = df.copy()\n",
        "\n",
        "# 1. Basic Metrics\n",
        "eda_df['log_return'] = np.log(eda_df['close'] / eda_df['close'].shift(1))\n",
        "eda_df['volatility_30d'] = eda_df['log_return'].rolling(window=24*30).std() * np.sqrt(24*365) # Annualized Vol\n",
        "eda_df['cum_return'] = (1 + eda_df['log_return']).cumprod()\n",
        "\n",
        "# 2. Time Features\n",
        "eda_df['hour'] = eda_df.index.hour\n",
        "eda_df['day_name'] = eda_df.index.day_name()\n",
        "eda_df['month'] = eda_df.index.month\n",
        "eda_df['year'] = eda_df.index.year\n",
        "\n",
        "# 3. Drawdown Analysis\n",
        "eda_df['running_max'] = eda_df['close'].cummax()\n",
        "eda_df['drawdown'] = (eda_df['close'] - eda_df['running_max']) / eda_df['running_max']\n",
        "\n",
        "# 4. Clean Data\n",
        "eda_df.dropna(inplace=True)\n",
        "\n",
        "# 5. Statistical Tests\n",
        "adf_result = adfuller(eda_df['close'])\n",
        "adf_pvalue = adf_result[1]\n",
        "adf_status = \"STATIONARY\" if adf_pvalue < 0.05 else \"NON-STATIONARY (Dangerous for ML)\"\n",
        "\n",
        "# --- B. DASHBOARD PLOTTING ---\n",
        "fig = plt.figure(constrained_layout=True)\n",
        "gs = fig.add_gridspec(6, 2)\n",
        "\n",
        "# PANEL 1: Price vs Volatility (Macro View)\n",
        "ax1 = fig.add_subplot(gs[0, :])\n",
        "color_price = '#1f77b4' # Bloomberg Blue\n",
        "ax1.plot(eda_df.index, eda_df['close'], color=color_price, alpha=0.8, label='Price', linewidth=1)\n",
        "ax1.set_ylabel('Price (USD)', color=color_price, fontweight='bold')\n",
        "ax1.tick_params(axis='y', labelcolor=color_price)\n",
        "ax1.set_title(f'1. Macro View: Price Action vs Market Fear (Volatility)', fontweight='bold')\n",
        "ax1.grid(True, which='major', linestyle='--', alpha=0.5)\n",
        "\n",
        "ax1_twin = ax1.twinx()\n",
        "color_vol = '#d62728' # Alert Red\n",
        "ax1_twin.set_ylabel('Annualized Volatility (30D)', color=color_vol, fontweight='bold')\n",
        "ax1_twin.plot(eda_df.index, eda_df['volatility_30d'], color=color_vol, alpha=0.3, linestyle='-', label='Volatility', linewidth=1)\n",
        "ax1_twin.tick_params(axis='y', labelcolor=color_vol)\n",
        "ax1_twin.fill_between(eda_df.index, 0, eda_df['volatility_30d'], color=color_vol, alpha=0.05)\n",
        "\n",
        "# PANEL 2: Equity Curve\n",
        "ax2 = fig.add_subplot(gs[1, 0])\n",
        "ax2.plot(eda_df.index, eda_df['cum_return'], color='#2ca02c', linewidth=1.5) # Profit Green\n",
        "ax2.set_title(f'2. Growth of $1 Investment (Total Return: {(eda_df[\"cum_return\"].iloc[-1]-1)*100:.2f}%)', fontweight='bold')\n",
        "ax2.fill_between(eda_df.index, 1, eda_df['cum_return'], alpha=0.1, color='#2ca02c')\n",
        "ax2.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 3: Underwater Plot\n",
        "ax3 = fig.add_subplot(gs[1, 1])\n",
        "ax3.plot(eda_df.index, eda_df['drawdown'], color='#d62728', linewidth=1)\n",
        "ax3.fill_between(eda_df.index, 0, eda_df['drawdown'], color='#d62728', alpha=0.2)\n",
        "ax3.set_title(f'3. Underwater Plot (Max Drawdown: {eda_df[\"drawdown\"].min()*100:.2f}%)', fontweight='bold')\n",
        "ax3.set_ylabel('Drawdown (%)')\n",
        "ax3.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 4: Monthly Seasonality\n",
        "ax4 = fig.add_subplot(gs[2, 0])\n",
        "monthly_returns = eda_df.groupby(['year', 'month'])['log_return'].sum().unstack()\n",
        "month_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
        "monthly_returns.columns = monthly_returns.columns.map(month_map)\n",
        "sns.heatmap(monthly_returns, annot=True, fmt='.1%', cmap='RdYlGn', center=0, ax=ax4, cbar=False, annot_kws={\"size\": 9})\n",
        "ax4.set_title('4. Monthly Seasonality (Calendar Heatmap)', fontweight='bold')\n",
        "\n",
        "# PANEL 5: Hourly Micro-Structure\n",
        "ax5 = fig.add_subplot(gs[2, 1])\n",
        "hourly_pivot = eda_df.groupby(['day_name', 'hour'])['log_return'].mean().unstack()\n",
        "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "hourly_pivot = hourly_pivot.reindex(days_order)\n",
        "sns.heatmap(hourly_pivot, cmap='RdYlGn', center=0, ax=ax5, cbar_kws={'label': 'Avg Return'})\n",
        "ax5.set_title('5. Intraday Strategy: Best Time to Trade', fontweight='bold')\n",
        "ax5.set_ylabel('')\n",
        "\n",
        "# PANEL 6: Fat Tail Analysis\n",
        "ax6 = fig.add_subplot(gs[3, 0])\n",
        "sns.histplot(eda_df['log_return'], bins=100, kde=True, color='#9467bd', ax=ax6, stat=\"density\", element=\"step\")\n",
        "mu, std = eda_df['log_return'].mean(), eda_df['log_return'].std()\n",
        "x = np.linspace(mu - 4*std, mu + 4*std, 100)\n",
        "p = (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * ((x - mu) / std)**2)\n",
        "ax6.plot(x, p, 'k--', linewidth=2, label='Normal Dist (Theory)')\n",
        "ax6.set_title(f'6. Risk Distribution (Kurtosis: {kurtosis(eda_df[\"log_return\"]):.2f})', fontweight='bold')\n",
        "ax6.set_xlim(-0.05, 0.05)\n",
        "ax6.legend()\n",
        "ax6.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 7: Autocorrelation\n",
        "ax7 = fig.add_subplot(gs[3, 1])\n",
        "pd.plotting.autocorrelation_plot(eda_df['log_return'].resample('1D').mean(), ax=ax7)\n",
        "ax7.set_title('7. Market Efficiency Test (Autocorrelation)', fontweight='bold')\n",
        "ax7.set_xlim(0, 30)\n",
        "ax7.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 8: Price-Volume Correlation Rolling\n",
        "ax8 = fig.add_subplot(gs[4, 0])\n",
        "rolling_corr = eda_df['close'].rolling(window=24*30).corr(eda_df['volume'])\n",
        "ax8.plot(eda_df.index, rolling_corr, color='#ff7f0e', linewidth=1)\n",
        "ax8.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "ax8.set_title('8. Trend Validation: Price-Volume Correlation (30D Rolling)', fontweight='bold')\n",
        "ax8.set_ylabel('Correlation')\n",
        "ax8.fill_between(eda_df.index, 0, rolling_corr, where=(rolling_corr>0), color='green', alpha=0.1)\n",
        "ax8.fill_between(eda_df.index, 0, rolling_corr, where=(rolling_corr<0), color='red', alpha=0.1)\n",
        "ax8.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 9: Risk per Day (Bar Chart)\n",
        "ax9 = fig.add_subplot(gs[4, 1])\n",
        "daily_vol = eda_df.groupby('day_name')['log_return'].std() * np.sqrt(24) * 100\n",
        "daily_vol = daily_vol.reindex(days_order)\n",
        "sns.barplot(x=daily_vol.index, y=daily_vol.values, ax=ax9, palette='viridis')\n",
        "ax9.set_title('9. Daily Volatility Profile (Risk by Day)', fontweight='bold')\n",
        "ax9.set_ylabel('Avg Daily Vol (%)')\n",
        "ax9.set_xticklabels(days_order, rotation=45)\n",
        "ax9.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# PANEL 10: Outlier Detection (Boxplot)\n",
        "ax10 = fig.add_subplot(gs[5, :])\n",
        "sns.boxplot(x='day_name', y='log_return', data=eda_df, order=days_order, ax=ax10, palette='Set2', fliersize=1)\n",
        "ax10.set_title('10. Weekly Anomaly Detection (Outliers)', fontweight='bold')\n",
        "ax10.set_ylim(-0.05, 0.05)\n",
        "ax10.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.suptitle(f'ADF Status: {adf_status} (p={adf_pvalue:.4f})', fontsize=24, y=1.01, fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "# --- D. TEXT SUMMARY ---\n",
        "print(\"\\nüìä --- EXECUTIVE QUANTITATIVE REPORT ---\")\n",
        "print(f\"1. MARKET REGIME\")\n",
        "print(f\"   - Volatility Trend      : {'Increasing' if eda_df['volatility_30d'].iloc[-1] > eda_df['volatility_30d'].mean() else 'Decreasing'}\")\n",
        "print(f\"   - Volume Confirmation   : {rolling_corr.iloc[-1]:.2f} (Correlation)\")\n",
        "print(f\"     (Positif = Healthy Trend , Negative/Zero = Weak Trend/Divergen)\")\n",
        "\n",
        "print(f\"\\n2. RISK CALENDAR\")\n",
        "print(f\"   - Riskiest Day          : {daily_vol.idxmax()} (Vol: {daily_vol.max():.2f}%)\")\n",
        "print(f\"   - Safest Day            : {daily_vol.idxmin()} (Vol: {daily_vol.min():.2f}%)\")\n",
        "print(f\"   - Best Month            : {monthly_returns.mean().idxmax()}\")\n",
        "\n",
        "print(f\"\\n3. MODELING INSIGHTS\")\n",
        "print(f\"   - Stationarity          : {adf_status} (Needs Differencing/Log Return)\")\n",
        "print(f\"   - Fat Tails             : Kurtosis {kurtosis(eda_df['log_return']):.2f} -> Needs Robust Scaling\")"
      ],
      "metadata": {
        "id": "Pn8Vl-MtjxFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DATA SCIENCE\n",
        "---"
      ],
      "metadata": {
        "id": "rxbaTm7arIw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "S58qJSnHjqnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def technical_indicators_engine(df):\n",
        "    data = df.copy()\n",
        "\n",
        "    # 1. TREND & MOMENTUM (Standard Indicators)\n",
        "    data['ema_50']  = data['close'].ewm(span=50, adjust=False).mean()\n",
        "    data['ema_200'] = data['close'].ewm(span=200, adjust=False).mean()\n",
        "    data['dist_ema_50']  = (data['close'] - data['ema_50']) / data['ema_50']\n",
        "    data['dist_ema_200'] = (data['close'] - data['ema_200']) / data['ema_200']\n",
        "\n",
        "    delta = data['close'].diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "    rs = gain / loss\n",
        "    data['rsi'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    min_rsi = data['rsi'].rolling(window=14).min()\n",
        "    max_rsi = data['rsi'].rolling(window=14).max()\n",
        "    data['stoch_rsi'] = (data['rsi'] - min_rsi) / (max_rsi - min_rsi)\n",
        "\n",
        "    exp12 = data['close'].ewm(span=12, adjust=False).mean()\n",
        "    exp26 = data['close'].ewm(span=26, adjust=False).mean()\n",
        "    data['macd'] = exp12 - exp26\n",
        "    data['macd_signal'] = data['macd'].ewm(span=9, adjust=False).mean()\n",
        "    data['macd_hist'] = data['macd'] - data['macd_signal']\n",
        "\n",
        "    data['bb_middle'] = data['close'].rolling(window=20).mean()\n",
        "    data['bb_std']    = data['close'].rolling(window=20).std()\n",
        "    data['bb_upper']  = data['bb_middle'] + (data['bb_std'] * 2)\n",
        "    data['bb_lower']  = data['bb_middle'] - (data['bb_std'] * 2)\n",
        "    data['bb_percent'] = (data['close'] - data['bb_lower']) / (data['bb_upper'] - data['bb_lower'])\n",
        "\n",
        "    high_low   = data['high'] - data['low']\n",
        "    high_close = np.abs(data['high'] - data['close'].shift())\n",
        "    low_close  = np.abs(data['low'] - data['close'].shift())\n",
        "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "    data['atr'] = tr.rolling(window=14).mean()\n",
        "    data['atr_pct'] = data['atr'] / data['close']\n",
        "\n",
        "    data['vol_change'] = data['volume'].pct_change()\n",
        "    data['vol_ma_20'] = data['volume'].rolling(window=20).mean()\n",
        "    data['vol_ratio'] = data['volume'] / data['vol_ma_20']\n",
        "\n",
        "    # --- HALVING AWARENESS ---\n",
        "    halving_dates = [\n",
        "        pd.Timestamp('2016-07-09'),\n",
        "        pd.Timestamp('2020-05-11'),\n",
        "        pd.Timestamp('2024-04-20'),\n",
        "        pd.Timestamp('2028-04-17')\n",
        "    ]\n",
        "\n",
        "    def get_days_since_halving(current_date):\n",
        "        past_halvings = [date for date in halving_dates if date <= current_date]\n",
        "        if not past_halvings:\n",
        "            # Fallback if the data older than 2016\n",
        "            return 0\n",
        "        last_halving = past_halvings[-1]\n",
        "        return (current_date - last_halving).days\n",
        "\n",
        "    data['days_since_halving'] = data.index.to_series().apply(get_days_since_halving)\n",
        "\n",
        "    # Normalize: The halving cycle is ~1460 days. Divide it so the number is 0.0 - 1.0\n",
        "    data['halving_progress'] = data['days_since_halving'] / 1460.0\n",
        "\n",
        "    # --- TARGET ENGINEERING ---\n",
        "    data['log_return'] = np.log(data['close'] / data['close'].shift(1))\n",
        "    data['target_return'] = data['log_return'].shift(-1)\n",
        "\n",
        "    # Dynamic Threshold (Smart Target)\n",
        "    data['dynamic_threshold'] = data['atr_pct'].rolling(window=24).mean() * 0.5\n",
        "    data['dynamic_threshold'].fillna(0.002, inplace=True)\n",
        "\n",
        "    data['target_class'] = np.where(data['target_return'] > data['dynamic_threshold'], 1, 0)\n",
        "\n",
        "    # CLEANUP\n",
        "    data.dropna(inplace=True)\n",
        "    data.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# APPLY\n",
        "print(\"Starting Feature Engineering...\")\n",
        "df_processed = technical_indicators_engine(df)\n",
        "print(f\"DONE!\")\n",
        "print(df_processed[['close', 'days_since_halving', 'halving_progress', 'target_class']].tail())"
      ],
      "metadata": {
        "id": "f5KH7KcLjxbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "PZlILvOOjsb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
        "\n",
        "# 1. SETUP DATASET\n",
        "drop_cols = ['target_class', 'target_return', 'log_return', 'open', 'high', 'low', 'close', 'dynamic_threshold']\n",
        "X = df_processed.drop(columns=drop_cols, errors='ignore')\n",
        "y = df_processed['target_class']\n",
        "\n",
        "# 2. TIME SERIES SPLIT\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
        "\n",
        "print(f\"Training Data: {len(X_train)} | Testing Data: {len(X_test)}\")\n",
        "\n",
        "# 3. HYPERPARAMETER TUNING\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 15, 20, None],\n",
        "    'min_samples_split': [10, 20, 50],\n",
        "    'min_samples_leaf': [5, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'class_weight': ['balanced', 'balanced_subsample']\n",
        "}\n",
        "\n",
        "# Model init\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Time Series Cross-Validation\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "print(\"\\nStarting Hyperparameter Tuning...\")\n",
        "\n",
        "# RandomizedSearchCV finding 10 random best combination\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=10,\n",
        "    scoring='precision',\n",
        "    cv=tscv,\n",
        "    verbose=3,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "best_model = search.best_estimator_\n",
        "print(f\"\\nDone!\")\n",
        "print(f\"Best Settings: {search.best_params_}\")\n",
        "\n",
        "# 4. EVALUATE\n",
        "print(\"\\n--- Best Model Perform (Data Test) ---\")\n",
        "preds = best_model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(f\"Accuracy: {acc:.2%}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, preds))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False)\n",
        "plt.xlabel('AI Predict')\n",
        "plt.ylabel('Reality')\n",
        "plt.title('Confusion Matrix (Optimized Model)')\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importances = best_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "top_n = 15\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.title(\"Decision-Making Features (Optimized)\")\n",
        "plt.barh(range(top_n), importances[indices[:top_n]], align=\"center\", color='#2ca02c')\n",
        "plt.yticks(range(top_n), [X.columns[i] for i in indices[:top_n]])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n",
        "\n",
        "# Update variabel global 'model'\n",
        "model = best_model"
      ],
      "metadata": {
        "id": "eYG7HXhajx6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction"
      ],
      "metadata": {
        "id": "d2KWtwGCjtui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_narrative(row, signal):\n",
        "    reasons = []\n",
        "\n",
        "    # Best Value\n",
        "    rsi = row['rsi'].values[0]\n",
        "    macd_hist = row['macd_hist'].values[0]\n",
        "    vol_ratio = row['vol_ratio'].values[0]\n",
        "    ema_dist = row['dist_ema_200'].values[0]\n",
        "    bb_pos = row['bb_percent'].values[0]\n",
        "\n",
        "    if signal == 1: # RECOMMENDATION BUY\n",
        "        if rsi < 30: reasons.append(\"RSI in Oversold territory; high probability of a technical rebound.\")\n",
        "        elif rsi > 50 and rsi < 70: reasons.append(\"Strong bullish RSI momentum; significant upside remains before Overbought.\")\n",
        "\n",
        "        if macd_hist > 0: reasons.append(\"Positive MACD Histogram confirms upward momentum.\")\n",
        "        if vol_ratio > 1.0: reasons.append(f\"Volume is {vol_ratio:.1f}x above average, validating move strength.\")\n",
        "        if ema_dist > 0: reasons.append(\"Price holding above EMA 200; Primary Trend is Bullish.\")\n",
        "        if bb_pos < 0.2: reasons.append(\"Price testing Lower Bollinger Band; potential mean reversion long.\")\n",
        "\n",
        "    else: # RECOMMENDATION WAIT/HOLD\n",
        "        if rsi > 70: reasons.append(\"RSI Overbought; high correction risk.\")\n",
        "        if macd_hist < 0: reasons.append(\"Negative MACD Histogram; momentum is fading.\")\n",
        "        if ema_dist < 0: reasons.append(\"Price below EMA 200; Primary Trend is Bearish.\")\n",
        "        if bb_pos > 0.8: reasons.append(\"Price near Upper Bollinger Band; asset is overextended.\")\n",
        "        if vol_ratio < 0.8: reasons.append(\"Anemic volume; lack of market conviction.\")\n",
        "\n",
        "    if not reasons:\n",
        "        reasons.append(\"Mixed technical signals. No clear edge detected.\")\n",
        "\n",
        "    return reasons\n",
        "\n",
        "def generate_signal_with_reasoning(model, df_processed, feature_cols):\n",
        "    latest_data = df_processed.iloc[[-1]].copy()\n",
        "    input_features = latest_data[feature_cols]\n",
        "\n",
        "    prediction = model.predict(input_features)[0]\n",
        "    probabilities = model.predict_proba(input_features)[0]\n",
        "    confidence = probabilities[prediction]\n",
        "\n",
        "    # Feature Importance\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    # Top 3 Features that most influence the model\n",
        "    top_drivers = []\n",
        "    for i in indices[:3]:\n",
        "        feature_name = feature_cols[i]\n",
        "        feature_val = input_features[feature_name].values[0]\n",
        "        top_drivers.append((feature_name, feature_val))\n",
        "\n",
        "    return latest_data, prediction, confidence, top_drivers\n",
        "\n",
        "feature_columns = X_train.columns\n",
        "latest_row, signal, conf, top_drivers = generate_signal_with_reasoning(model, df_processed, feature_columns)\n",
        "\n",
        "current_price = latest_row['close'].values[0]\n",
        "timestamp     = latest_row.index[0]\n",
        "rsi_now       = latest_row['rsi'].values[0]\n",
        "\n",
        "print(\"--- REPORT ---\")\n",
        "print(f\"Time (UTC)    : {timestamp}\")\n",
        "print(f\"Asset         : {SYMBOL}\")\n",
        "print(f\"Current Price : ${current_price:,.2f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# RECOMMENDATION\n",
        "if signal == 1:\n",
        "    print(f\"SIGNAL     : üü¢ BUY / LONG\")\n",
        "    color_code = \"üü¢\"\n",
        "else:\n",
        "    print(f\"SIGNAL     : üî¥ WAIT / HOLD (No Entry)\")\n",
        "    color_code = \"üî¥\"\n",
        "\n",
        "print(f\"Confidence : {conf*100:.2f}%\")\n",
        "if conf < 0.6:\n",
        "    print(\"Risk Note  : Confidence < 60%. Weak Signal, be carefull.\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# \"WHY\"\n",
        "print(\"BASIS OF DECISION:\")\n",
        "\n",
        "# Narrative Logic\n",
        "narratives = generate_narrative(latest_row, signal)\n",
        "for i, reason in enumerate(narratives, 1):\n",
        "    print(f\"   {i}. {reason}\")\n",
        "\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Top Technical Drivers\n",
        "print(\"KEY INDICATORS WATCHED (Faktor Dominan):\")\n",
        "for name, val in top_drivers:\n",
        "    print(f\"   ‚Ä¢ {name.ljust(15)} : {val:.4f}\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Disclaimer: Statistical probability only. Not financial advice.\")"
      ],
      "metadata": {
        "id": "rcs9RDeljyVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}